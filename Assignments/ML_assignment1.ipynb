{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Algorithm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_wine_data():\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "    data = pd.read_csv(url, sep=\";\")\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data):\n",
    "    X = data.drop('quality', axis=1)\n",
    "    y = data['quality']\n",
    "    y = (y >= 6).astype(int)\n",
    "    scaler = StandardScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "def train_naive_bayes(X_train, y_train):\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def predict_and_evaluate(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy, y_pred\n",
    "\n",
    "def main():\n",
    "    data = load_wine_data()\n",
    "    X, y = preprocess_data(data)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    \n",
    "    clf = train_naive_bayes(X_train, y_train)\n",
    "    \n",
    "    accuracy, y_pred = predict_and_evaluate(clf, X_test, y_test)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Belief Network\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import ExpectationMaximization\n",
    "from pgmpy.inference import VariableElimination\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "df['target'] = wine.target\n",
    "\n",
    "df.columns = [col.replace(' ', '_').replace('/', '_') for col in df.columns]\n",
    "\n",
    "print(\"Feature names in the dataset:\", df.columns.tolist())\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans')\n",
    "df[df.columns[:-1]] = discretizer.fit_transform(df[df.columns[:-1]])\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "mi_scores = mutual_info_classif(train_data.drop(columns='target'), train_data['target'])\n",
    "feature_ranking = pd.Series(mi_scores, index=train_data.columns[:-1]).sort_values(ascending=False)\n",
    "top_features = feature_ranking.index[:5]\n",
    "\n",
    "model = BayesianNetwork()\n",
    "for feature in top_features:\n",
    "    model.add_node(feature)\n",
    "model.add_node('target')\n",
    "model.add_node('alcohol')\n",
    "model.add_node('malic_acid')\n",
    "model.add_node('ash')\n",
    "model.add_node('alcalinity_of_ash')\n",
    "model.add_node('magnesium')\n",
    "model.add_node('total_phenols')\n",
    "model.add_node('flavanoids')\n",
    "model.add_node('nonflavanoid_phenols')\n",
    "model.add_node('proanthocyanins')\n",
    "model.add_node('color_intensity')\n",
    "model.add_node('hue')\n",
    "model.add_node('proline')\n",
    "\n",
    "for feature in top_features:\n",
    "    model.add_edge(feature, 'target')\n",
    "\n",
    "model.fit(train_data, estimator=ExpectationMaximization)\n",
    "\n",
    "infer = VariableElimination(model)\n",
    "\n",
    "def predict(instance):\n",
    "    evidence = instance.drop('target').to_dict()\n",
    "    query_result = infer.query(variables=['target'], evidence=evidence, show_progress=False)\n",
    "    return query_result.values.argmax()\n",
    "\n",
    "test_data['Predicted'] = test_data.apply(predict, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(test_data['target'], test_data['Predicted'])\n",
    "print(f'Accuracy of the Bayesian Belief Network Classifier: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Candidate Elimination Algo\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
    "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
    "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
    "    'Play Tennis': [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def candidate_elimination(data, target_attribute):\n",
    "    # Specific boundary (most specific hypothesis)\n",
    "    specific = ['?'] * (len(data.columns) - 1)\n",
    "\n",
    "    # General boundary (most general hypothesis)\n",
    "    general = [['?'] * (len(data.columns) - 1)]\n",
    "\n",
    "    # Convert data to lists of tuples, excluding the target attribute column\n",
    "    examples = data.drop(target_attribute, axis=1).to_records(index=False)\n",
    "    labels = data[target_attribute].tolist()\n",
    "\n",
    "    for index, instance in enumerate(examples):\n",
    "        instance = tuple(instance)\n",
    "        label = labels[index]\n",
    "\n",
    "        if label == 1:  # If the instance is positive\n",
    "            for i, val in enumerate(specific):\n",
    "                if specific[i] == '?':\n",
    "                    specific[i] = instance[i]\n",
    "                elif specific[i] != instance[i]:\n",
    "                    specific[i] = '?'\n",
    "            general = [g for g in general if any(specific[i] == '?' or specific[i] == g[i] for i in range(len(g)))]\n",
    "        else:  # If the instance is negative\n",
    "            general = [g for g in general if not all(specific[i] == '?' or specific[i] == g[i] or g[i] == instance[i] for i in range(len(g)))]\n",
    "            new_general = []\n",
    "            for g in general:\n",
    "                for i, val in enumerate(g):\n",
    "                    if val == '?':\n",
    "                        continue\n",
    "                    temp = list(g)\n",
    "                    temp[i] = '?' if specific[i] == instance[i] else specific[i]\n",
    "                    new_general.append(tuple(temp))\n",
    "            general.extend(new_general)\n",
    "\n",
    "    return specific, general\n",
    "\n",
    "# Run the Candidate-Elimination algorithm\n",
    "specific, general = candidate_elimination(df, 'Play Tennis')\n",
    "\n",
    "print(f'Specific boundary: {specific}')\n",
    "print(f'General boundary: {general}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apriori Algo\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "def load_data(sample_size=None):\n",
    "    # Load the Online Retail dataset\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"\n",
    "    df = pd.read_excel(url, engine='openpyxl')\n",
    "    \n",
    "    # Clean the data\n",
    "    df['Description'] = df['Description'].str.strip()\n",
    "    df.dropna(subset=['InvoiceNo', 'Description'], inplace=True)\n",
    "    \n",
    "    # Convert InvoiceNo to string and remove credit notes (invoices starting with 'C')\n",
    "    df['InvoiceNo'] = df['InvoiceNo'].astype(str)\n",
    "    df = df[~df['InvoiceNo'].str.startswith('C')]\n",
    "    \n",
    "    # Group the data by InvoiceNo and create a list of items for each transaction\n",
    "    transactions = df.groupby('InvoiceNo')['Description'].apply(list).reset_index()\n",
    "    \n",
    "    # Sample the transactions if a sample size is specified\n",
    "    if sample_size:\n",
    "        transactions = transactions.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    return transactions['Description'].tolist()\n",
    "\n",
    "def apply_apriori(transactions, min_support=0.01):\n",
    "    # Convert transactions to a one-hot encoded DataFrame\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    # Apply Apriori algorithm\n",
    "    frequent_itemsets = apriori(df, min_support=min_support, use_colnames=True)\n",
    "    return frequent_itemsets\n",
    "\n",
    "def find_most_frequent_itemsets(frequent_itemsets):\n",
    "    max_support = frequent_itemsets['support'].max()\n",
    "    most_frequent = frequent_itemsets[frequent_itemsets['support'] == max_support]\n",
    "    return most_frequent\n",
    "\n",
    "def main():\n",
    "    print(\"Loading and processing the Online Retail dataset...\")\n",
    "    try:\n",
    "        transactions = load_data(sample_size=5000)  # Sample 5000 transactions for efficiency\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total number of transactions: {len(transactions)}\")\n",
    "    if transactions:\n",
    "        print(f\"Sample transaction: {transactions[0][:5]}...\")  # Show first 5 items of the first transaction\n",
    "    else:\n",
    "        print(\"No transactions found.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nApplying Apriori algorithm...\")\n",
    "    try:\n",
    "        frequent_itemsets = apply_apriori(transactions, min_support=0.05)  # Increased min_support to reduce complexity\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying Apriori algorithm: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nTotal number of frequent itemsets found: {len(frequent_itemsets)}\")\n",
    "    print(\"\\nTop 10 frequent itemsets by support:\")\n",
    "    print(frequent_itemsets.sort_values(by='support', ascending=False).head(10))\n",
    "\n",
    "    most_frequent = find_most_frequent_itemsets(frequent_itemsets)\n",
    "    print(\"\\nMost frequent itemset(s):\")\n",
    "    print(most_frequent)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
